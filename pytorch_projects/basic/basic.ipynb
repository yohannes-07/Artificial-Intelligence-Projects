{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pytorch_logo.png/800px-Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pytorch Basics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some \"Matrices\" as lists of lists  \n",
    "\n",
    "# 3x3\n",
    "W = [[1, 1, 1],\n",
    "     [1.5, 1.5, 1.5],\n",
    "     [2, 2, 2]]\n",
    "\n",
    "# 3x1\n",
    "x = [[6], [7], [8]]\n",
    "# 3x1\n",
    "b = [[1], [1], [1]]\n",
    "\n",
    "# Variable to store output\n",
    "# 3x1\n",
    "y = [[0], [0], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can transform our list of lists into a \"numpy array\" by using the function \"array\"\n",
    "W_np = np.array(W)\n",
    "\n",
    "x_np = np.array(x)\n",
    "\n",
    "# Lets use the function \"ones\" to create an array of ones!\n",
    "b_np = np.ones((3, 1))\n",
    "\n",
    "# Lets now compute Wx + b using these numpy variables!\n",
    "output = np.matmul(W_np, x_np) + b_np\n",
    "\n",
    "# Print out the result\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Output shape:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can transform our list of lists into a \"torch tensor\" by using the function \"FloatTensor\"\n",
    "# Note: here we've specified the datatype of the tensor, a 32bit \"float\" you can also just use the function \"tensor\"\n",
    "# But this will inherit the datatype of the array given, to ensure the data-types are the same\n",
    "# (and we can perform the wanted operations) we use \"FloatTensor\"\n",
    "\n",
    "W_torch = torch.FloatTensor(W)\n",
    "\n",
    "x_torch = torch.FloatTensor(x)\n",
    "\n",
    "# Lets use the function \"ones\" to create an array of ones!\n",
    "b_torch = torch.ones(3, 1)\n",
    "\n",
    "# Lets now compute Wx + b using these numpy variables!\n",
    "output = torch.matmul(W_torch, x_torch) + b_torch\n",
    "\n",
    "# Print out the result\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Output shape:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Numpy and Pytorch are remarkably similar, though this is no coincidence! The creators of Pytorch did this intentionally to make it easy to transfer existing skills in Numpy (a Python library that everyone uses - has its origins back in 1995!!) to Pytorch. To aid this transfer there are even functions that can transfer Pytorch tensors to Numpy arrays and back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random Numpy array\n",
    "np_array = np.random.random((3, 4))\n",
    "print(\"Numpy array:\\n\", np_array)\n",
    "\n",
    "# Convert to Pytorch tensor\n",
    "torch_tensor = torch.FloatTensor(np_array)\n",
    "print(\"Pytorch tensor:\\n\", torch_tensor)\n",
    "\n",
    "# Convert back to a Numpy array!\n",
    "np_array2 = torch_tensor.numpy()\n",
    "print(\"Numpy array:\\n\", np_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>On to Pytorch!</h1>\n",
    "Let's further explore Pytorch and it's similarities to Numpy and then see what new functionalities it brings to the table!!\n",
    "<h3> Basic Element-wise Operations </h3>\n",
    "Let's quickly go back over some basics using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a 2D Tensor using torch.rand\n",
    "y = torch.rand(4, 5)\n",
    "# This will create a \"Vector\" of numbers from 0 to 1\n",
    "print(\"Our 1D Tensor:\\n\",y)\n",
    "\n",
    "# We can perform normal python scalar arithmetic on Torch tensors\n",
    "print(\"\\nScalar Multiplication:\\n\",y * 10)\n",
    "print(\"Addition and Square:\\n\",(y + 1)**2)\n",
    "print(\"Addition:\\n\",y + y)\n",
    "print(\"Addition and division:\\n\",y / (y + 1))\n",
    "\n",
    "# We can use a combination of Torch functions and normal python arithmetic\n",
    "print(\"\\nPower and square root:\\n\",torch.sqrt(y**2))\n",
    "\n",
    "# Torch tensors are objects and have functions\n",
    "print(\"\\nY -\\n Min:%.2f\\n Max:%.2f\\n Standard Deviation:%.2f\\n Sum:%.2f\" %(y.min(), y.max(), y.std(), y.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensor Opperations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two 3D Tensors\n",
    "tensor_1 = torch.rand(3,3,3)\n",
    "tensor_2 = torch.rand(3,3,3)\n",
    "\n",
    "# Add the 2 Tensors\n",
    "print(\"Addition:\\n\",tensor_1 + tensor_2)\n",
    "\n",
    "# We cannot perform a normal \"matrix\" multiplication on a 3D tensor\n",
    "# But we can treat the 3D tensor as a \"batch\" (like a stack) of 2D tensors\n",
    "# And perform normal matrix multiplication independantly on each pair of 2D matricies\n",
    "print(\"Batch Multiplication:\\n\", torch.bmm(tensor_1, tensor_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a more interesting tensor\n",
    "tensor_3 = torch.rand(2,4,5)\n",
    "# We can swap the Tensor dimensions\n",
    "print(\"\\nThe origional Tensor is is:\\n\", tensor_3)\n",
    "print(\"With shape:\\n\", tensor_3.shape)\n",
    "\n",
    "# Tranpose will swap the dimensions it is given\n",
    "print(\"The Re-arranged is:\\n\", tensor_3.transpose(0,2))\n",
    "print(\"With shape:\\n\", tensor_3.transpose(0,2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Indexing </h3>\n",
    "Indexing in Pytorch works the same as it does in Numpy, see if you can predict what values will be return by the indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4D Tensor\n",
    "tensor = torch.rand(2,3,1,4)\n",
    "print(\"Our Tensor:\\n\",tensor)\n",
    "\n",
    "# Select the last element of dim0\n",
    "print(\"\\nThe last element of dim0:\\n\",tensor[-1])\n",
    "\n",
    "# 1st element of dim0\n",
    "# 2nd element of dim1\n",
    "print(\"\\nIndexed elements:\\n\",tensor[0, 1])\n",
    " \n",
    "# Select all elements of dim0\n",
    "# The 2nd element of dim1\n",
    "# The 1st element of dim2\n",
    "# The 3rd element of dim3\n",
    "print(\"\\nIndexed elements:\\n\",tensor[:, 1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Describing Tensors </h3> <br>\n",
    "Lets see how we can view the characteristics of our Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a large 4D Tensor\n",
    "tensor = torch.rand(3, 5, 3, 2)\n",
    "\n",
    "# View the Number of elements in every dimension\n",
    "print(\"The Tensor's shape is:\", tensor.shape)\n",
    "\n",
    "# In Pytorch shape and size() do the same thing!\n",
    "print(\"The Tensor's shape using size() is:\", tensor.size())\n",
    "\n",
    "# View the number of elements in total\n",
    "print(\"There are %d elements in total:\" % tensor.numel())\n",
    "\n",
    "# View the number of Dimensions\n",
    "print(\"There are %d Dimensions\" %(tensor.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Reshaping </h3> <br>\n",
    "We can change a Tensor to one of the same size (same number of elements) but a different shape by using functions in a similar fashion to Numpy but with different functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us reshape our Tensor to a 2D Tensor\n",
    "print(\"Reshape to 3x30:\\n\", tensor.reshape(3, 30))\n",
    "\n",
    "# We can also use the Flatten method to convert to a 1D Tensor\n",
    "print(\"Flatten to a 1D Tensor:\\n\",tensor.flatten())\n",
    "\n",
    "# Here the -1 tells Pytorch to put as many elements as it needs here in order to maintain the given dimention sizes\n",
    "# AKA \"I don't care the size of this dimention as long as the first one is 10\"\n",
    "print(\"Reshape to 10xwhatever:\\n\",tensor.reshape(10, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Squeezing and Unsqueezing </h4>\n",
    "A very common shape-changing operation is to add an \"empty\" dimension to ensure the shape (specifically the number of dimensions) of the tensor is correct for certain functions. <br>\n",
    "For example, when we start using Pytorch Neural Network modules, we need to provide the input of the network with a \"batch\" dimension (we often pass multiple inputs to our network at once) even if we only pass 1 datapoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a 2D Tensor\n",
    "tensor = torch.rand(3, 2)\n",
    "\n",
    "# View the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", tensor.shape)\n",
    "\n",
    "# Unsqueeze adds an \"empty\" dimension to our Tensor\n",
    "print(\"Add an empty dimenson to dim3:\", tensor.unsqueeze(2).shape)\n",
    "\n",
    "# Unsqueeze adds an \"empty\" dimension to our Tensor\n",
    "print(\"Add an empty dimenson to dim2:\", tensor.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a 4D Tensor with a few \"empty\" dimensions\n",
    "tensor = torch.rand(1, 3, 1, 2)\n",
    "\n",
    "# view the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", tensor.shape)\n",
    "\n",
    "# squeeze removes an \"empty\" dimension from our Tensor\n",
    "print(\"Remove empty dimenson dim3:\", tensor.squeeze(2).shape)\n",
    "\n",
    "# squeeze removes an \"empty\" dimension from our Tensor\n",
    "print(\"Remove empty dimenson dim0:\", tensor.squeeze(0).shape)\n",
    "\n",
    "# If we don't specify a dimension, squeeze will remove ALL empty dimensions\n",
    "print(\"Remove all empty dimensons:\", tensor.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Broadcasting </h3>\n",
    "Broadcasting also works in Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create 2 differently shaped 4D Tensors (Matrices)\n",
    "tensor1 = torch.rand(1, 4, 3, 1)\n",
    "tensor2 = torch.rand(3, 4, 1, 4)\n",
    "\n",
    "print(\"Tensor 1 shape:\\n\", tensor1.shape)\n",
    "print(\"Tensor 2 shape:\\n\", tensor2.shape)\n",
    "\n",
    "tensor3 = tensor1 + tensor2\n",
    "\n",
    "print(\"The resulting shape is:\\n\", tensor3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pytorch Autograd </h1>\n",
    " So how does it do it? Well first Pytorch keeps track of everything we do!! (unless we tell it not to) It does this by forming a \"computational graph\" - a tree-like structure of all the operations we perform starting at some initial tensor. When we tell Pytorch to backpropagate from some point, it works backwards up this tree and calculates and stores the gradients with respect to the point from where we back propagated from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see an example of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some tensors, requires_grad tells Pytorch we want to store the gradients for this tensor\n",
    "# we need to do this if we are working with basic Pytorch tensors\n",
    "x = torch.FloatTensor([4])\n",
    "x.requires_grad = True\n",
    "w = torch.FloatTensor([2])\n",
    "w.requires_grad = True\n",
    "b = torch.FloatTensor([3])\n",
    "b.requires_grad = True\n",
    "\n",
    "# By performing a simple computation Pytorch will build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# It's easy to see that\n",
    "# dy/dx = w = 2\n",
    "# dy/dw = x = 4\n",
    "# dy/db = 1\n",
    "\n",
    "# Compute gradients via Pytorch's Autograd\n",
    "y.backward()\n",
    "\n",
    "# Print out the calculated gradients\n",
    "# These gradients are the gradients with respect to the point where we backprop'd from - y\n",
    "# Create your own equation and use the auto backprop to see the partial derivatives!\n",
    "print(\"Calculated Gradients\") \n",
    "print(\"dy/dx\", x.grad.item())    # x.grad = dy/dx = 2 \n",
    "print(\"dy/dw\", w.grad.item())    # w.grad = dy/dw = 4\n",
    "print(\"dy/db\", b.grad.item())   # b.grad = dy/db = 1  \n",
    "# Note: .item() simply returns a 0D Tensor as a Python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finding the minimum </h3>\n",
    "We can use gradient decent to find the minimum of an equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find the minimum of a parabola!\n",
    "\n",
    "# Define the equation as a lambda function\n",
    "fx = lambda  x: x**2 + 1.5 * x - 1\n",
    "\n",
    "x = np.linspace(-10, 8.5, 100)\n",
    "\n",
    "plt.plot(x, fx(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random point X\n",
    "x_ = torch.randn(1)\n",
    "x_.requires_grad = True\n",
    "\n",
    "# Lets use Pytorch's Autograd to find the gradient at this point\n",
    "y_ = fx(x_)\n",
    "y_.backward()\n",
    "\n",
    "# The gradient tells us the direction to travel to increase Y\n",
    "dy_dx_ = x_.grad.item()\n",
    "print(\"dy/dx is %.2f when x is %.2f\" % (dy_dx_, x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take some steps to decend the gradient!\n",
    "\n",
    "# Create a random point X\n",
    "x_ = torch.randn(1)\n",
    "x_.requires_grad = True\n",
    "\n",
    "# Create some loggers\n",
    "x_logger = []\n",
    "y_logger = []\n",
    "\n",
    "# We'll keep track of how many steps we've done\n",
    "counter = 0\n",
    "\n",
    "# Set a scale for the step size\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialise the gradient to a large value\n",
    "dy_dx_ = 1000\n",
    "\n",
    "# We'll limit the max number of steps so we don't create an infinite loop\n",
    "max_num_steps = 1000\n",
    "\n",
    "# Keep taking steps untill the gradient is small\n",
    "while np.abs(dy_dx_) > 0.001:\n",
    "    # Get the Y point at the current x value\n",
    "    y_ = fx(x_)\n",
    "    \n",
    "    # Calculate the gradient at this point\n",
    "    y_.backward()\n",
    "    dy_dx_ = x_.grad.item()\n",
    "\n",
    "    # Pytorch will not keep track of operations within a torch.no_grad() block\n",
    "    # We don't want Pytorch to add our gradient decent step to the computational graph!\n",
    "    with torch.no_grad():\n",
    "        # Take a step down (decend) the curve\n",
    "        x_ -= learning_rate * dy_dx_\n",
    "        \n",
    "        # Pytorch will accumulate the gradient over multiple backward passes\n",
    "        # For our use case we don't want this to happen so we need to set it to zero\n",
    "        # After we have used it\n",
    "        x_.grad.zero_()\n",
    "        \n",
    "        # Log the X and Y points to plot\n",
    "        x_logger.append(x_.item())\n",
    "        y_logger.append(y_.item())\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "    if counter == max_num_steps:\n",
    "        break\n",
    "\n",
    "print(\"Y minimum is %.2f and is when X = %.2f, found after %d steps\" % (y_.item(), x_.item(), counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Models and Optimizers</h1>\n",
    "\n",
    "<h2>Pytorch nn.Module</h2>\n",
    "In Pytorch the basic template for creating our models is the \"Module\" class within torch.nn. To create our own class we inherit this class as the \"superclass\" so that we have access to all the properties and functions. <br>\n",
    "Lets create our own constructor of this class!\n",
    "\n",
    "The two main functions we need to create are the <b>\\__init__</b> and <b>forward</b> functions. We've already seen <b>\\__init__</b> so lets looks at <b>forward</b><br>\n",
    "\n",
    "The <b>forward</b> function is the only function that we MUST create when we build our class, Pytorch uses this fuction as the \"entry point\" to our model and is what is called when we do a forward pass of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple implementation of an nn.Module subclass\n",
    "    Takes the input (x) and returns x * 4 + 2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Pass our class and self to superclass and call the superclass's init function\n",
    "        super(SimpleFunction, self).__init__() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * 4 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our class\n",
    "simple_function = SimpleFunction()\n",
    "# Perform a \"forward pass\" of our class\n",
    "output = simple_function(10)\n",
    "print(\"Class output:\", output)\n",
    "# Note we do NOT need to explicitly call the .forward() function of our class,\n",
    "# a forward pass of our models is such a common step that Pytorch makes it easier and cleaner for us to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A more complicated model </h3><br>\n",
    "The previous nn.Module class that we created wasn't really a ML \"model\", lets create something that we've seen before; a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the input (x) and returns x * w^t + b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Pass our class and self to superclass and call the superclass's init function\n",
    "        super(LinearModel, self).__init__() \n",
    "        # nn.Parameter wraps our normal tensors and \"tells\" Pytorch\n",
    "        # that they are our nn.Module's model parameters to be optimized \n",
    "        self.w = nn.Parameter(torch.randn(output_size, input_size))\n",
    "        self.b = nn.Parameter(torch.randn(1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x,  self.w.t()) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of 10 datapoints each 5D\n",
    "input_data = torch.randn(10, 5)\n",
    "\n",
    "# Create an instance of our Model\n",
    "linear_model = LinearModel(5, 1)\n",
    "\n",
    "# Perform a forward pass!\n",
    "output = linear_model(input_data)\n",
    "\n",
    "print(output.shape)\n",
    "print(output.detach())\n",
    "# Note: detach \"disconnects\" the tensor and returns it with no history of previous calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pytorch inbuilt Neural Network Layers</h2>\n",
    "The \"Linear layer\" is so common place that Pytorch already has an implementation of it, in fact Pytorch has implementations of most Layer types which act as building blocks for our multi-layer models. For now lets just see how we can implement Pytorch's linear layer (we will see may more layer types later in the semester!).<br>\n",
    "<b>Things to know!</b><br>\n",
    "- Pytorch initialises the weights and biases of it's layers in very particular ways (not just from a normal distribution!), usualy based off of deep learning research, see the documentation for more details.<br>\n",
    "- Pytorch includes a bias term in it's layers by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear layer aka a \"fully connected\" layer aka a \"Perceptron\" layer\n",
    "# nn.Linear(Number of inputs, Number of outputs) \n",
    "linear = nn.Linear(3, 1) \n",
    "\n",
    "# Lets have a look at the parameters of this layer\n",
    "# The \"weights\" are what is multipied by the input data\n",
    "print ('w:\\n', linear.weight.data)\n",
    "# The bias is then added on!\n",
    "print ('b:\\n', linear.bias.data)\n",
    "\n",
    "print ('w shape:\\n', linear.weight.data.shape)\n",
    "print ('b shape:\\n', linear.bias.data.shape)\n",
    "# Note: .data just gives us the raw Tensor without any connection to the computational graph\n",
    "# - it looks nicer when we print it out\n",
    "# Note: The opperation the linear layer performs is y = x*A^t + b\n",
    "# where A^t is the transpose of the weights and b is the bias,\n",
    "# this opperation is also know as an \"affine transformation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the gradients of these parameters\n",
    "print ('w:\\n', linear.weight.grad)\n",
    "print ('b:\\n', linear.bias.grad)\n",
    "# Note: Pytorch initialises the grad of the tensors to \"None\" NOT 0!\n",
    "# They only get created after the first backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random data input tensor\n",
    "data = torch.randn(100, 3)\n",
    "# Create some noisey target data\n",
    "target = data.sum(1, keepdims=True) + 0.01*torch.randn(data.shape[0], 1)\n",
    "print ('Input data:\\n', data[:10])\n",
    "print ('Output data:\\n', target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, lets perform a \"forward pass\" of our model, aka let's put the data into the model and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember! To perform a forward pass of our model, we just need to \"call\" our network\n",
    "# Pytorch's nn.Module class will automatically pass it to the \"forward\" function in the layer class\n",
    "target_pred = linear(data)\n",
    "print(\"Network output:\\n\", target_pred.data[:10])\n",
    "print(\"Network output shape:\", target_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss Functions and Optimizers</h3>\n",
    "Now lets see how Pytorch helps us optimize our model!<br>\n",
    "<b>Loss functions</b><br>\n",
    "We've already seen loss function's before and defined our own, but using Pytorch we can pick from some pre-defined functions (we can also just create our own).\n",
    "\n",
    "<b>Optimizers</b><br>\n",
    "This is the object that will be doing the parameter updates for us! Pytorch has a number of different optimizers, some of which we will explore in future labs. For now we will just use our well known Gradient Descent (GD) optimizer.<br>\n",
    "Note: Most optimizers are just some variant of GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform a regression with a mean square error loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Lets create a Stochastic gradient descent optimizer with a learning rate of 0.01\n",
    "# (the way we will be using it, it is just normal GD) \n",
    "# When we create the optimizer we need to tell it WHAT it needs to optimize, so the first thing \n",
    "# We pass it are the linear layer's \"parameters\"\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the following scatter plot that the outout of our model is NOT the same as our target data, let's see what the MSE loss is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first dimension of the input vs the output\n",
    "\n",
    "# Use the outputs of the model from a few cells ago\n",
    "plt.scatter(data[:, 0], target_pred.detach())\n",
    "# Use the Ground Truth data\n",
    "plt.scatter(data[:, 0], target, marker=\"x\")\n",
    "plt.legend([\"Predictions\", \"Ground Truth Data\"])\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Ouputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(target_pred, target)\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform a backward pass of our model to compute the gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "# Note for every backwards pass of the model we must first perform a forward pass\n",
    "# as data from parts of the computational graph have been deleted upon the backward pass to save memory.\n",
    "# We can tell Pytorch to hold onto this data, but, in many cases it needs to be recalculated anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, tell the optimizer to perform an update step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he critical step to update the parameter which reduce the loss\n",
    "optimizer.step()\n",
    "\n",
    "# Perform another forward pass of the model to check the new loss\n",
    "target_pred = linear(data)\n",
    "loss = loss_function(target_pred, target)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Training Loop</h3>\n",
    "Our loss has gone down!! Lets see how low we can get it to go by constructing a training loop!<br>\n",
    "For MOST tasks (but not all) a simgle training iteration in Pytorch can be summarised in the following 5 steps:<br>\n",
    "- Forward pass of our model with the data.<br>\n",
    "- Calculate the loss.<br>\n",
    "- Reset the current stored gradients to 0<br>\n",
    "- Backpropagate the loss to calculate the new gradients.<br>\n",
    "- Perform an optimization step.<br>\n",
    "<br>\n",
    "We perform these steps over and over until our model has converged or some other point has been reached (depending on the application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create an empty array to log the loss\n",
    "loss_logger = []\n",
    "\n",
    "# Lets perform 100 itterations of our dataset\n",
    "for i in range(1000):\n",
    "    # Perform a forward pass of our data\n",
    "    target_pred = linear(data)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_function(target_pred, target)\n",
    "    \n",
    "    # .zero_grad sets the stored gradients to 0\n",
    "    # If we didn't do this they would be added to the \n",
    "    # Gradients from the previous step!\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate the new gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform an optimization step!\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_logger.append(loss.item())\n",
    "    \n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets graph out the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first dimension of the input vs the output\n",
    "plt.scatter(data[:, 0], target_pred.detach())\n",
    "plt.scatter(data[:, 0], target, marker=\"x\")\n",
    "plt.legend([\"Predictions\", \"Ground Truth\"])\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Ouputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
