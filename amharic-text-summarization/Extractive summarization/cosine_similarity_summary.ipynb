{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19f1555-625c-445d-b342-461a9f8d00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yohannes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = open('stopwords.txt', encoding='utf-8').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eff0c22-e802-46da-a97e-d85f21c89b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('../cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3674b640-736d-4824-a305-456c05c22ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>የአማራ ብሄራዊ ክልል መንግስት መግለጫ</td>\n",
       "      <td>የሀገራችን ህዝቦች በአብሮነትና በአንድነት በህብረ ብሄራዊነት ለበርካታ አ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>አዴፓዎች ለዶክተር አብይ ግልፅ ቅድመ ሁኔታዎች ማስቀመጥ አለባቸው ግርማ ካሳ</td>\n",
       "      <td>በታገቱ ተማሪዎችና በሌሎች የአገር ደህንነትና መከላከያ ጉዳዮች ዙሪያ አዴ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>በተማሪዎች ላይ የተፈፀመውን የእገታ ድርጊት የሚያወግዙ ሰላማዊ ሰልፎች በ...</td>\n",
       "      <td>ባህር ዳር ሰላማዊ ሰልፎቹ በደምቢ ዶሎ ዩኒቨርሲቲ ሲማሩ የነበሩና የታገቱ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>በአቶ ገዱ አንዳርጋቸው የተመራ የልኡካን ቡድን ወደ አሜሪካ ተጓዘ</td>\n",
       "      <td>በኢፌዲሪ የውጭ ጉዳይ ሚኒስትር አቶ ገዱ አንዳርጋቸው የተመራ የኢትዮጵያ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ጦቢያ ግጥምን በጃዝ   መላኩ ታረቀኝ</td>\n",
       "      <td>ጦቢያ ግጥምን በጃዝ  መላኩ ታረቀኝ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ዶክተር አብይ ሱዳንን አሻገረ   የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው</td>\n",
       "      <td>ዶክተር አብይ ሱዳንን አሻገረ  የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>በነገራችን ላይ አንዳርጋቸው ፅጌ</td>\n",
       "      <td>በነገራችን ላይ አንዳርጋቸው ፅጌ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ</td>\n",
       "      <td>ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>የአሜሪካ ድምፅ ራዲዮ ከአቶ ንጉሱ ጥላሁን እና ከሆኑት አቶ ነቢያት ጌታቸ...</td>\n",
       "      <td>የአሜሪካ ድምፅ ራዲዮ የጠቅላይ ሚኒስትር ፅህፈት ቤት ፕሬስ ሴክሬታሪያት ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ይድርስ በኢትዮጵያ ለምትገኙ የዩኒቨርስቲ ተማሪዎች</td>\n",
       "      <td>በገክርስቶስ አባይጥር  ቀን  አመተ ምህረትከሁሉ አስቀድሜ ሰላምና ጤና እ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           የአማራ ብሄራዊ ክልል መንግስት መግለጫ   \n",
       "1   አዴፓዎች ለዶክተር አብይ ግልፅ ቅድመ ሁኔታዎች ማስቀመጥ አለባቸው ግርማ ካሳ   \n",
       "2  በተማሪዎች ላይ የተፈፀመውን የእገታ ድርጊት የሚያወግዙ ሰላማዊ ሰልፎች በ...   \n",
       "3          በአቶ ገዱ አንዳርጋቸው የተመራ የልኡካን ቡድን ወደ አሜሪካ ተጓዘ   \n",
       "4                            ጦቢያ ግጥምን በጃዝ   መላኩ ታረቀኝ   \n",
       "5       ዶክተር አብይ ሱዳንን አሻገረ   የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው   \n",
       "6                               በነገራችን ላይ አንዳርጋቸው ፅጌ   \n",
       "7   ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ   \n",
       "8  የአሜሪካ ድምፅ ራዲዮ ከአቶ ንጉሱ ጥላሁን እና ከሆኑት አቶ ነቢያት ጌታቸ...   \n",
       "9                    ይድርስ በኢትዮጵያ ለምትገኙ የዩኒቨርስቲ ተማሪዎች   \n",
       "\n",
       "                                             content  \n",
       "0  የሀገራችን ህዝቦች በአብሮነትና በአንድነት በህብረ ብሄራዊነት ለበርካታ አ...  \n",
       "1  በታገቱ ተማሪዎችና በሌሎች የአገር ደህንነትና መከላከያ ጉዳዮች ዙሪያ አዴ...  \n",
       "2  ባህር ዳር ሰላማዊ ሰልፎቹ በደምቢ ዶሎ ዩኒቨርሲቲ ሲማሩ የነበሩና የታገቱ...  \n",
       "3  በኢፌዲሪ የውጭ ጉዳይ ሚኒስትር አቶ ገዱ አንዳርጋቸው የተመራ የኢትዮጵያ ...  \n",
       "4                            ጦቢያ ግጥምን በጃዝ  መላኩ ታረቀኝ   \n",
       "5       ዶክተር አብይ ሱዳንን አሻገረ  የሀገሬውም ህዝብ ለምስጋና እጅ ነሳው   \n",
       "6                              በነገራችን ላይ አንዳርጋቸው ፅጌ   \n",
       "7  ጠቅላይ ሚኒስትር ዶክተር አቢይ አህመድ ከህግ ባለሙያዎች ጋር ውይይት አካሄዱ   \n",
       "8  የአሜሪካ ድምፅ ራዲዮ የጠቅላይ ሚኒስትር ፅህፈት ቤት ፕሬስ ሴክሬታሪያት ...  \n",
       "9  በገክርስቶስ አባይጥር  ቀን  አመተ ምህረትከሁሉ አስቀድሜ ሰላምና ጤና እ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0152156c-b483-429e-a0ff-efa50d3236f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineModel:\n",
    "\n",
    "    am_sent_endings = r'\\?|\\!|\\።|\\፡፡'\n",
    "    am_punctuation = '፠፡።፣፤፥፦፧፨“”‘’…‹‹››·•'\n",
    "    am_numbers = '፩፪፫፬፭፮፯፰፱፲፳፴፵፶፷፸፹፺፻፼'\n",
    "    am_random = '�©\\uf0c4\\uf0d8\\uf0a7\\uf066\\uf0d8'\n",
    "    stop_words = open('stopwords.txt', encoding='utf-8').read().split()\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.raw_text = text\n",
    "        self.clean_text = None\n",
    "        self.sentences = None\n",
    "        self.words = None\n",
    "\n",
    "        self.process_text()\n",
    "\n",
    "    def process_text(self):\n",
    "        self.clean_text = self.clean_minimized(self.raw_text)\n",
    "        self.sentences = self.extract_sentences(self.clean_text)\n",
    "        self.sentences = self.remove_duplicate_sentence(self.sentences)\n",
    "        self.words = self.extract_words(self.clean_text)\n",
    "\n",
    "    def extract_sentences(self, text=None):\n",
    "        if text is None:\n",
    "            text = self.raw_text\n",
    "        sentences = re.split(self.am_sent_endings, text)\n",
    "        return sentences\n",
    "\n",
    "    def extract_words(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def clean_minimized(self, text):\n",
    "        words = text.split()\n",
    "        to_clean = string.punctuation + self.am_numbers + self.am_random + string.ascii_letters + string.digits + self.am_punctuation\n",
    "        to_clean = re.sub(self.am_sent_endings, '', to_clean)\n",
    "        table = str.maketrans('', '', to_clean)\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        clean_txt = list(filter(None, stripped))\n",
    "        return ' '.join(clean_txt)\n",
    "\n",
    "    def remove_duplicate_sentence(self, sentences):\n",
    "        duplicates = []\n",
    "        cleaned = []\n",
    "        for s in sentences:\n",
    "            if s in cleaned:\n",
    "                if s in duplicates:\n",
    "                    continue\n",
    "                else:\n",
    "                    duplicates.append(s)\n",
    "            else:\n",
    "                cleaned.append(s)\n",
    "        return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e0207d-01a1-4f62-a561-8fa6aa738a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: etnltk in /home/yohannes/.local/lib/python3.10/site-packages (0.0.22)\n",
      "Requirement already satisfied: emoji>=1.7.0 in /home/yohannes/.local/lib/python3.10/site-packages (from etnltk) (2.9.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/yohannes/.local/lib/python3.10/site-packages (from etnltk) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in /home/yohannes/.local/lib/python3.10/site-packages (from textsearch>=0.0.21->etnltk) (2.0.0)\n",
      "Requirement already satisfied: anyascii in /home/yohannes/.local/lib/python3.10/site-packages (from textsearch>=0.0.21->etnltk) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install etnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5dd839-4442-44c4-8929-604505025d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similarities(text):\n",
    "#     sentences = remove_stopwords(text)\n",
    "#     sentences = sent_tokenize(str(text))\n",
    "#     print(sentences)\n",
    " \n",
    "#     vectorizer = TfidfVectorizer()\n",
    "   \n",
    "#     trsfm = vectorizer.fit_transform(sentences)\n",
    "#     text_df = pd.DataFrame(trsfm.toarray(), columns=vectorizer.get_feature_names_out(), index=sentences)\n",
    "\n",
    "#     num_sentences = text_df.shape[0]\n",
    "#     num_summary_sentences = int(np.ceil(num_sentences ** 0.5))\n",
    "\n",
    "#     similarities = cosine_similarity(trsfm, trsfm)\n",
    "\n",
    "#     avgs = []\n",
    "#     for i in similarities:\n",
    "#         avgs.append(i.mean())\n",
    "\n",
    "    \n",
    "#     top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "\n",
    "#     return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e0feea9-be35-464f-b8a0-31ef6d2ab226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_summary(text):\n",
    "#     sents_for_sum = find_similarities(text)\n",
    "#     sort = sorted(sents_for_sum)\n",
    "   \n",
    "#     print('Number of selected sentences', len(sort))\n",
    "#     sent_list = sent_tokenize(str(text))\n",
    "#     print('Total number of sentences', len(sent_list))\n",
    "\n",
    "#     sents = []\n",
    "#     for i in sort:\n",
    "#         sents.append(sent_list[i].replace('\\n', '።') + '።')\n",
    "\n",
    "#     summary = ' '.join(sents)\n",
    "#     return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed5639e-89e5-4ef4-b89b-df2875495d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(sentences, stopwords):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "    trsfm = vectorizer.fit_transform(sentences)\n",
    "    text_df = pd.DataFrame(trsfm.toarray(), columns=vectorizer.get_feature_names_out(), index=sentences)\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences ** .5))\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    " \n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "   \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6864d1a0-d597-412b-8bdd-75ce4af3a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary(sentences, stop_words):\n",
    "    sents_for_sum = find_similarities(sentences, stopwords=stop_words)\n",
    "    sort = sorted(sents_for_sum)\n",
    "    print('Number of selected sentences', len(sort))\n",
    "    sent_list = sentences\n",
    "    print(sentences)\n",
    "    print('Total number of sentences', len(sent_list))\n",
    "   \n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', '') + '።')\n",
    "    summary = ' '.join(sents)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c256b484-67cf-4cb3-84d2-f2830e338535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import etnltk\n",
    "from etnltk.tokenize.am import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89ede2e-ffb3-4c64-8b5a-c9392cc97eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def longest_common_subsequence(a, b):\n",
    "    matcher = SequenceMatcher(None, a, b)\n",
    "    match = matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "    return a[match.a: match.a + match.size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0f3537c-fe3f-4517-8542-5e9add190977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(reference, candidate, n=1):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "    \n",
    "    reference_ngrams = set(nltk.ngrams(reference_tokens, n))\n",
    "    candidate_ngrams = set(nltk.ngrams(candidate_tokens, n))\n",
    "    \n",
    "    precision = len(reference_ngrams.intersection(candidate_ngrams))\n",
    "    recall = len(reference_ngrams) + len(candidate_ngrams) - precision\n",
    "    \n",
    "    if recall == 0:\n",
    "        rouge_n_score = 0.0\n",
    "    else:\n",
    "        rouge_n_score = precision / recall\n",
    "    \n",
    "    return rouge_n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec942cef-992f-43e8-80e9-0957dbc6b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_l(reference, candidate):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "    \n",
    "    reference_ngrams_all = list(nltk.ngrams(reference_tokens, 1))\n",
    "    candidate_ngrams_all = list(nltk.ngrams(candidate_tokens, 1))\n",
    "    \n",
    "    reference_lcs = longest_common_subsequence(reference_ngrams_all, candidate_ngrams_all)\n",
    "    \n",
    "    rouge_l_score = len(reference_lcs) / max(1, len(reference_ngrams_all))\n",
    "    \n",
    "    return rouge_l_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad53d2ac-0190-4fdb-997a-cdbb55d25f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(reference, candidate):\n",
    "    rouge_n_1 = rouge_n(reference, candidate, n=1)\n",
    "    rouge_n_2 = rouge_n(reference, candidate, n=2)\n",
    "    rouge_l_score = rouge_l(reference, candidate)\n",
    "    \n",
    "    return rouge_n_1, rouge_n_2, rouge_l_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc4c827c-d419-446f-a4a7-ed9c52b2fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(reference_summary, generated_summary):\n",
    "    rouge_n_1, rouge_n_2, rouge_l_score = calculate_rouge_scores(reference_summary, generated_summary)\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_n_1:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_n_2:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd98cff-cb22-4d99-bc24-781d2f5b71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_data['content']\n",
    "y = cleaned_data['title']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa850467-e9ff-46c1-b74b-a9cf55920999",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94764837-64fc-47be-9e49-d565e6c4c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected sentences 5\n",
      "['በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች', ' ደካማ የሚባል አቋም ያሳየችው ካሜሩን ሻንጣዋን ልትሸክፍ ግድ ሆኗል', ' የጣሊያኑ አታላንታ አጥቂ ለአጋማሽ ዘጠኝ ደቂቃዎች ሲቀሩት ባስቆጠራት ጎል ሀገሩን ቀዳሚ አድርጓል', ' ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል', ' የቀድሞው ከ ዓመት በታች የእንግሊዝ ብሔራዊ ቡድን አጥቂ የነበረው ሉክማን ሁለተኛዋን ጎሎ በኛው ደቂቃ በማስቆጠር የካሜሩንን ልብ ሰብሯል', ' ምንም እንኳ በጉዳት ምክንያት ሳይሰለፍ የቆየው የካሜሩኑ አጥቂ ቪንሴንት አቡበከር ዘግይቶ ወደ ሜዳ ቢገባም ሀገሩ አንድ ዒላማውን የጠበቀ ኳስ ሳታስመዘግብ ወጥታለች', ' አቡበከር ባለፈው ካሜሩን ባዘጋጀችው የ የአፍሪካ ዋንጫ ኮከብ ጎል አስቆጣሪ ነበር', ' በዓለም ዋንጫ ከምድቡ ማለፍ ያልቻለው የካሜሩኑ አሠልጣኝ ሪጎቤርት ሶንግ ጫናው እየበረታ መጥቷል', ' የካሜሩን እግር ኳስ ፌዴሬሽን ፕሬዝደንት ሳሙዔል ኤቶ ጨዋታውን በሥፍራው ተገኝቶ የተከታተለ ሲሆን የቀድሞ የቡድን አጋሩን ሥራ የመታደግ አሊያም የማባረር ኃላፊነት አለበት', ' በዘንድሮው ውድድር እምብዛም አስደናቂ ብቃት ያላሳዩት የማይበገሩት አናብስት ከናይጄሪያ በነበራቸውም ጨዋታው ከቅርፅ ውጭ ሆነው ተስተውለዋል', ' በተቃራኒው ናይጄሪያ ያገኘችው ውጤት የሚገባት የሚያስብል ሲሆን ተጋጣሚዋን በልጣ ታይታለች', ' በዘጠነኛው ደቂቃ የናይጄሪያው ተከላካይ ሴሚ አጃይ ያስቆጠራት ጎል በቪኤአር ታይታ ውድቅ ሆናበታለች', ' የካሜሩንን የመጀመሪያ ጨዋታ ለማንቸስተር ዩናይትድ ለመጫወት ሲል የቀጣው ግብ ጠባቂው አንድሬ ኦናና ለቀሪዎቹ ጨዋታዎች በግል አውሮፕላን ወደ አይቮሪ ኮስት ቢበርም መሰለፍ የቻለው አንድ ጨዋታ ብቻ ነው', ' እሱን ተክቶ የተሰለፈው ኦንዶዋ ስኅተት በፈፀመ ቁጥር ካሜራው ኦናናን ሲያስመልክት ተስተውሏል', ' በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች', ' የናሚቢያው ግብ ጠባቂ ኔብሉ በኛው ደቂቃ በቀጥታ ቀይ ካርድ ከሜዳ ተሰናብቷል', ' ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል', ' ዳላ ለራሱ እና ለሀገሩ ሁለተኛዋን ጎል ሲያስቆጥር ማቡሉሉ ደግሞ በአስደናቂ ሁኔታ ሶስተኛዋን በማከል አንጎላ ሩብ ፍፃሜውን እንድትቀላቀል አስችለዋል', ' አንጎላ በመጪው አርብ በሩብ ፍፃሜው ናይጄሪያን ትገጥማለች', '']\n",
      "Total number of sentences 20\n",
      "ROUGE-1: 0.9000\n",
      "ROUGE-2: 0.8235\n",
      "ROUGE-L: 0.7049\n",
      "\n",
      "\n",
      "\n",
      "labeled summary \n",
      " በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። \n",
      " ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  \n",
      " በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
      "       \n",
      "\n",
      "\n",
      "Generated Summary በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች።  ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  የናሚቢያው ግብ ጠባቂ ኔብሉ በኛው ደቂቃ በቀጥታ ቀይ ካርድ ከሜዳ ተሰናብቷል።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n"
     ]
    }
   ],
   "source": [
    "content = '''\n",
    "በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። ደካማ የሚባል አቋም ያሳየችው ካሜሩን ሻንጣዋን ልትሸክፍ ግድ ሆኗል። የጣሊያኑ አታላንታ አጥቂ ለአጋማሽ ዘጠኝ ደቂቃዎች ሲቀሩት ባስቆጠራት ጎል ሀገሩን ቀዳሚ አድርጓል። \n",
    "ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል። የቀድሞው ከ21 ዓመት በታች የእንግሊዝ ብሔራዊ ቡድን አጥቂ የነበረው ሉክማን ሁለተኛዋን ጎሎ በ90ኛው ደቂቃ በማስቆጠር የካሜሩንን ልብ ሰብሯል።\n",
    "ምንም እንኳ በጉዳት ምክንያት ሳይሰለፍ የቆየው የካሜሩኑ አጥቂ ቪንሴንት አቡበከር ዘግይቶ ወደ ሜዳ ቢገባም ሀገሩ አንድ ዒላማውን የጠበቀ ኳስ ሳታስመዘግብ ወጥታለች። አቡበከር ባለፈው ካሜሩን ባዘጋጀችው የ2021 የአፍሪካ ዋንጫ ኮከብ ጎል አስቆጣሪ ነበር። በዓለም ዋንጫ ከምድቡ ማለፍ ያልቻለው የካሜሩኑ አሠልጣኝ ሪጎቤርት ሶንግ ጫናው እየበረታ መጥቷል።\n",
    "የካሜሩን እግር ኳስ ፌዴሬሽን ፕሬዝደንት ሳሙዔል ኤቶ ጨዋታውን በሥፍራው ተገኝቶ የተከታተለ ሲሆን የቀድሞ የቡድን አጋሩን ሥራ የመታደግ አሊያም የማባረር ኃላፊነት አለበት። በዘንድሮው ውድድር እምብዛም አስደናቂ ብቃት ያላሳዩት የማይበገሩት አናብስት ከናይጄሪያ በነበራቸውም ጨዋታው ከቅርፅ ውጭ ሆነው ተስተውለዋል።\n",
    "በተቃራኒው ናይጄሪያ ያገኘችው ውጤት የሚገባት የሚያስብል ሲሆን ተጋጣሚዋን በልጣ ታይታለች። በዘጠነኛው ደቂቃ የናይጄሪያው ተከላካይ ሴሚ አጃይ ያስቆጠራት ጎል በቪኤአር ታይታ ውድቅ ሆናበታለች።\n",
    "የካሜሩንን የመጀመሪያ ጨዋታ ለማንቸስተር ዩናይትድ ለመጫወት ሲል የቀጣው ግብ ጠባቂው አንድሬ ኦናና ለቀሪዎቹ ጨዋታዎች በግል አውሮፕላን ወደ አይቮሪ ኮስት ቢበርም መሰለፍ የቻለው አንድ ጨዋታ ብቻ ነው። እሱን ተክቶ የተሰለፈው ኦንዶዋ ስኅተት በፈፀመ ቁጥር ካሜራው ኦናናን ሲያስመልክት ተስተውሏል።\n",
    "በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን 3 ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች። የናሚቢያው ግብ ጠባቂ ኔብሉ በ17ኛው ደቂቃ በቀጥታ ቀይ ካርድ ከሜዳ ተሰናብቷል። ጌልሰን ዳላ ከ9 ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
    "ዳላ ለራሱ እና ለሀገሩ ሁለተኛዋን ጎል ሲያስቆጥር ማቡሉሉ ደግሞ በአስደናቂ ሁኔታ ሶስተኛዋን በማከል አንጎላ ሩብ ፍፃሜውን እንድትቀላቀል አስችለዋል። አንጎላ በመጪው አርብ በሩብ ፍፃሜው ናይጄሪያን ትገጥማለች።\n",
    "        '''\n",
    "\n",
    "title  = '''\n",
    " በአፍሪካ ዋንጫ ናይጄሪያ በአዴሞላ ሉክማን ሁለት ጎሎች ካሜሩንን በመርታት ወደ ሩብ ፍፃሜው ተሸጋገረች። \n",
    " ሉክማን ከናፖሊው አጥቂ ቪክተር ኦሲምሄን ያገኘውን ኳስ ኦናናን ተክቶ የተሰለፈው ግብ ጠባቂው ፋብሪስ ኦንደዋን ተሻግሮ አስቆጥሯል።  \n",
    " በሌላ ጨዋታ አንጎላ ሁለት ተጫዋች በቀይ ያጣችውን ናሚቢያን ለምንም በመርታት ወደ ሩብ ፍፃሜው ተቀላቅላለች።  ጌልሰን ዳላ ከ ደቂቃ በኋላ ለአንጎላ ጎል ካስቆጠረ በኋላ የናሚቢያው ተከላካይ ሉቤኒ ሃውኮንጎ በሁለት ቢጫ ካርድ ከሜዳ ተባሯል።\n",
    "       '''\n",
    "tparser = CosineModel(content)\n",
    "article_summary = build_summary(tparser.sentences, tparser.stop_words)\n",
    "rouge_score(title, article_summary)\n",
    "print(\"\\n\\n\")\n",
    "print(\"labeled summary\", title)\n",
    "print(\"\\n\")\n",
    "print(\"Generated Summary\", article_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e1a8b-8fd1-4139-8adc-f4d37b38a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "የተቀናጀ (Integrated) የሚለው ቃል በጥሬው ሲተረጎም፣ የተለያዩ ነገሮችን በማጣመር ወይም በማዋሃድ የተስማማና ግንኙነቱ የጠበቀ ሙሉ ነገርን መፍጠር ማለት ነው፡፡ ወደ ተቀናጀ ቅድመ አንደኛ ደረጃ ትምህርት ስንመጣ የቅንጅቱ መነሻ፣ የተቀረፁት ጭብጦችና ለዚህ ማስተላለፊያ ተብለው የተዘጋጁት የመማር ዓውዶች ይሆናል፡፡ በዚህ ሒደት ስለቅንጅት ስናወራ በቀጥታ ወይም በተዘዋዋሪ መንገድ ስለጭብጥና የመማር ዓውድ እናወራለን ማለት ነው፡፡ ስለዚህ የቅድመ አንደኛ ደረጃ ሥርዓተ ትምህርት ቅንጅት በአብዛኛው የሚወድቀው በጭብጡና በመማር ዓውዱ፣ እንዲሁም በመማር ዓውዱ ንዑሳን ክፍሎች መካከል ይሆናል ማለት ነው፡፡ ከዚህ የሚነሱ የተለያዩ የቅንጅት ዓይነቶች ይገኛሉ፡፡ ከነዚህም ውስጥ፣ በቅድመ አንደኛ ደረጃ ትምህርት ሥርዓተ ትምህርት አደረጃጀት ውስጥ በድግግሞሽ የሚነሱትና የሚተገበሩት ሁለት ናቸው፡፡ እነሱም፣ ውጫዊ ቅንጅት (Inter-disciplinary) እና ውስጣዊ ቅንጅት (Intra-Disciplinary) የሚባሉት ናቸው፡፡ \n",
    "\n",
    "'''\n",
    "# input_text = \"ተዘግቶ የነበረው መጋረጃ ሲገለጥ የሚል ርእስ የሰጠሁበት ዋናው ምክንያት እነ ሻምበል አምሃ አበበና የመሳሰሉት በመቶዎች የሚቆጠሩ ብዙ ወጣት መኮንኖች የበታች ሹሞችና ወታደሮች በኮርስ ወይም በቅርብ ጓደኞቻቸው የሀሰት ጥቆማ ተካሂዶባቸው ህይወታቸው የተቀጨበት አካለ ስንኩላን የሆኑበት ምክንያት ብዙ ሚስጢር መውጣት በመጀመሩ ነው። ኮሎኔል መርሻ ወዳጆ በመጽሐፉ ሽፋን ላይ ስለመጽሐፉ የሚከተለውን አስተያየት ሰጥተዋል። ጄኔራል አማን የጦር አለቆችን ማነጋገር መጀመራቸው የደርግ አባላቱ ሲረዱ የፍርሃትና የጭንቀት ብርድ ውስጥ ከተታቸው። ደህንነቱ ልዩ ልዩ የጦር ክፍሎችና ከፖሊስ ሠራዊት ተዛውረን የመጣን ሲሆን ከመሥሪያ ቤቴ የሰው ኃይል ሲነፃፀር ብዛት ባይኖራቸውም በተለያዩ የትምህርት ዘርፎች በዶክትሬት በማስትሬት በባችለር ዲግሪ ከዚያም ዝቅ ሲል በዲፕሎማ የተመረቁ በተጨማሪ ብዛት ያላቸው ሠራተኞች ወደ ተለያዩ ሶሻሊስት አገሮች ተልከው የመረጃና የሶሻሊስት ርእዮተ ዓለም ትምህርት የተክታተሉ አባሎች እንደነበሩ እሙን ነው። በሰሜን በኩል የነበረው የኢትዮጵያ ጦር መፈረካከስና መበተን የጀመረው ከግንቦት ቀን በኋላ እንደሆነ ግልጽ ነው። አስተያየት በእርግጥም የውጭ መረጃ ሠራተኞች ጂቡቲ ሆነው የሱማሌን መንግሥት የጦር ቢሮ ቦርቡረው ውጤት አስገኝተዋል የሚለው እውነትነት ካለው ሁላችንንም ማለትም በደህነነት መቤት ውስጥ የነበርነውን ሠራተኞች ሁሉ የሚያኮራን ነው። ከ ዓም ወዲህ የሻዕቢያና የ ወያኔ ቡድን አባሎች በጋራና በተናጠል አገር ውስጥ በስውር ይንቀሳቀሱ እንደነበር በተመሳሳይ ጂቡቲ ኬኒያ ሱማሌና በተለይም ሱዳን ውስጥ ጽሕፈት ቤቶቻቸውን ከፍተው በከፍተኛ ደረጃ ፀረ ኢትዮጵያ እንቅስቃሴ ሲያደርጉ እንደነበር ይታወቃል። ሻምበል ተስፋዬ ርስቴ በጻፉት መጽሐፍ በገጽ የውጭ አገሮች ጥናትና ምርምር ድርጅት ውስጥ እየተገናኙ ውይይት ያደርጉ እንደነበር ገልጸዋል። ስለዚህ የፕሬዚዳንት መንግሥቱ ኃማርያም ማንነትና የሚስጥር አማካሪዎቻቸው እነማን እንደነበሩ ከላይ የተገለጸው ጥሩ ማስረጃ ነው እላለሁ። በእርግጥ ከላይ ስማችሁ የተገለጸው መኮንኖች የአንድ ኮርስ መኮንኖች መሆናችሁን በወቅቱ ሻምበል ቁምላቸው ተካና ሻምበል አመሀ አበበ አንድ ቤት ውስጥ ተከራይተው ይኖሩ እንደነበር ይነገራል።\"\n",
    "# input_text = \"በኢትዮጵያ በጥሬ ገንዘብ የሚደረጉ የገንዘብ እንቅስቃሴዎች እየቀነሱ መሆኑን መረጃዎች እያመለከቱ ነው፡፡ በአብዛኛዎቹ የኢትዮጵያ ባንኮች የሚደረጉ የገንዘብ እንቅስቃሴዎች በተለያዩ የዲጂታል አማራጮች እየተተገበሩ ነው፡፡ በተለይ በ2015 ሒሳብ ዓመት እንደ ሪከርድ የሚቆጠር አፈጻጸም መታየቱም ይገልጻል፡፡ በ2016 ሒሳብ ዓመትም ቢሆን በዲጂታል ባንክ አገልግሎት የሚንቀሳቀሱ የገንዘብ መጠን እያደገ ስለመምጣቱ እየወጡ ያሉ መረጃዎች እያመላከቱ ነው፡፡ በጥሬ ገንዘብ የሚደረግ ግብይት በከፍተኛ ደረጃ እየቀነሰ መምጣቱ ተጠቆመ:: የኢትዮጵያ ብሔራዊ ባንክ ይፋዊ መረጃ እንደሚያሳየውም፣ በተጠናቀቀው የ2015 ሒሳብ ዓመት በዲጂታል የክፍያ አማራጮች የተንቀሳቀሰው የገንዘብ መጠን ከ4.7 ትሪሊዮን ብር በላይ ደርሷል፡፡ ይህ የገንዘብ መጠን ከቀዳሚው ዓመት የ2014 የሒሳብ ዓመት ጋር ሲነፃፀር፣ ከ3.1 ትሪሊዮን ብር በላይ ብልጫ አለው፡፡ በግብይት መጠንም ቢሆን በ2014 የሒሳብ ዓመት የተፈጸመው ትራንዛክሽን ወይም ግብይት 345.7 ሚሊዮን ሲሆን፣ በ2015 ሒሳብ ዓመት ግን 1.24 ቢሊዮን ደርሷል፡፡ \"\n",
    "# input_text = \"በኢትዮጵያ ንግድ ባንክ መረጃ መሠረት በ2016 የሒሳብ ዓመት ግማሽ ዓመት ብቻ ከ2.8 ትሪሊዮን ብር በላይ በዲጂታል የባንክ አገልግሎት ገንዘብ ማንቀሳቀስ ችሏል፡፡ ይህ ገንዘብ እንቅስቃሴ ከ494 ሚሊዮን ብር በላይ በሚሆኑ ግብይቶች ወይም ትራንዛሽኖች የተፈጸመ ነው፡፡ ይህ አፈጻጸም ከቀዳሚው ዓመት ተመሳሳይ ወቅት ጋር ሲነፃፀር፣ በግብይት የ35 በመቶ በገንዘብ መጠን ደግሞ የ118 በመቶ ዕድገት የታየበት ነው፡፡ በቀዳሚው ዓመት በሙሉ የሒሳብ ዓመት 3.2 ትሪሊዮን ብር ማንቀሳቀስ ችሎ የነበረው የኢትዮጵያ ንግድ ባንክ፣ የስድስት ወራት 2.8 ቢሊዮን ብር ማንቀሳቀስ የቻለው ከ12 ሚሊዮን በላይ የኤቲኤም ተጠቃሚ፣ ከዘጠኝ ሚሊዮን በላይ የሞባይል ባንክና ከ20 ሚሊዮን ብር በላይ በሚሆኑ ደንበኞች በመያዝ እንደሆነ ታውቋል፡፡\"\n",
    "\n",
    "tparser = CosineModel(input_text)\n",
    "article_summary = build_summary(tparser.sentences, tparser.stop_words)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Text\")\n",
    "print(\"\\n\")\n",
    "print(input_text)\n",
    "print(\"===================\")\n",
    "print(\"Summarized\")\n",
    "print(\"\\n\")\n",
    "print(article_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3807f50-b81a-4390-bae8-f894ddfd5b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
